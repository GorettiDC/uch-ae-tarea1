{"cells":[{"cell_type":"markdown","metadata":{"id":"1yxbELmQZOmi"},"source":["# Diplomado de Inteligencia Artificial\n","Por Alexandre Bergel (abergel@dcc.uchile.cl) y Juan-Pablo Silva.\n","\n","Integrantes del grupo:\n","\n","\n","*  Goretti De Castro\n","*   Yerko Ortiz M.\n","*   Integrante 3\n","*   Integrante 4\n","\n","\n","\n","## Reinforcement Learning\n","\n","El reinforcement learning es una tecnica en el aprendizaje de maquinas donde no damos feedback inmediatas a las maquinas, en este contexto comunmente referidas **agentes**, sino que dejamos se exploren y se desenvuelvan en el mundo. Luego, mientras se desenvuelven, damos recompensas si hacen una buena accion, y una penalizacion si hacen algo malo. Esto suena muy similar al aprendizaje supervisado que ya han visto con redes neuronales, pero hay una diferencia bastante sustancial.\n","\n","Las redes neuronales en aprendizaje supervisado reciben feedback, o una evaluacion de lo que han hecho, por **cada** accion que realizan. En un video juego por ejemplo, es dificil saber si dar un paso hacia adelante es algo positivo o negativo, por lo tanto es muy dificil saber si la accion es correcta o no; ese paso hacia adelante pudo haber influido en hacernos ganar, o perder el juego, imposible saber en ese momento.\n","En cambio, con reinforcement learning damos feedback por acciones **buenas** y castigamos acciones **malas**, no nos preocupamos por acciones *intermedias* que no estamos seguros a donde nos pueden llevar, eso es parte de lo que el agente debe aprender. Es por esto que se llama *aprendizaje reforzado*, u otro nombre quizas mas adecuado, aprendizaje con feedback retardado, porque le decimos despues de que hizo las acciones si estas estan bien o no.\n","\n","## Actividades\n","\n","En este *notebook* hay varias actividades que realizaremos en los 2 dias que dura el topico de *Reinforcement Learning*. Aqui encontras preguntas que debes responder, y ejercicios que contaran como nota del topico. Estas secciones con nota seran etiquetadas con **Pregunta**.\n","\n","Primero introduciremos el ambiente en el que nos desenvolveremos. Existen librerias como *Gym* que presentan ambientes de videojuegos para probar y evaluar tecnicas de *reinforcemente learning*, pero son una caja negra para la mayoria de los casos y entrar al codigo es bastante complejo. Para mitigar esto, hemos diseÃ±ado nuestro propio ambiente, muy simple, que nos permitira jugar un pequeÃ±o juego donde tenemos un **heroe** ğŸ™ƒ que necesita recoger **trofeos** ğŸ† y evitar a los **zombies** ğŸ§Ÿ."]},{"cell_type":"markdown","metadata":{"id":"pPmfPbg8bVSE"},"source":["## Ambiente para nuestro agente\n","\n","Aqui implementaremos las partes necesarias para que nuestro agente pueda vivir en este mundo.\n","\n","\n","Primero necesitamos importar algunas cosas para facilitarnos la vida."]},{"cell_type":"code","execution_count":90,"metadata":{"executionInfo":{"elapsed":721,"status":"ok","timestamp":1725892522801,"user":{"displayName":"Alexandre Bergel","userId":"02563535896661050156"},"user_tz":-120},"id":"fJaoDoPXMzCQ"},"outputs":[],"source":["# copy para copiar nuestros objetos de un lado para otro\n","from copy import deepcopy, copy\n","# numpy es una libreria numerica que permite facil trabajo con\n","# matrices, como matlab. La usamos para trabajos numericos\n","import numpy as np\n","# para tener cosas random!\n","import random\n","# para ayudarnos con los tipos, nos ayuda a tener mas claro que retorna que\n","from typing import List, Tuple, Any, Union, NewType, Dict\n","\n","# siempre es bueno usar una semilla cuando hacemos experimentos, es la unica\n","# forma confiable que tenemos para asegurarnos que nuestros experimentos\n","# son reproducibles\n","random.seed(42)  # no importa que numero elijamos, pero lo dejamos fijo"]},{"cell_type":"code","execution_count":91,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1725892523442,"user":{"displayName":"Alexandre Bergel","userId":"02563535896661050156"},"user_tz":-120},"id":"jFBSvEohM8FI"},"outputs":[],"source":["# Vamos a definir algunas cosas con `monitos` para que nuestro mapa se\n","# vea mas entretenido\n","\n","# Partiremos con estos\n","ZOMBIE = \"ğŸ§Ÿ\"\n","HERO = \"ğŸ™ƒ\"\n","TROPHIE = \"ğŸ†\"\n","EMPTY = \"âšª\"\n","\n","# despues ustedes tendran que agregar estos!\n","BLOCK = \"ğŸš«\"\n","KEY = \"ğŸ”‘\"\n","DOOR = \"ğŸšª\"\n","SWORD = \"ğŸ—¡ï¸\"\n","\n","\n","# Nuestro agente tiene que saber las condiciones del mundo donde existe\n","# en este mundo solo hay 4 acciones que puede hacer.\n","# Moverse hacia `arriba`, `abajo`, `derecha` e `izquierda`, nada mas.\n","# En algun otro ambiente, podriamos agregar mas acciones como saltar\n","# atacar, comprar, etc. Pero mantendremos la simplicidad aqui.\n","UP = 0\n","DOWN = 1\n","LEFT = 2\n","RIGHT = 3\n","# Juntamos nuestras acciones para que queden ordenadas.\n","ACTIONS = [UP, DOWN, LEFT, RIGHT]"]},{"cell_type":"code","execution_count":92,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1725892523442,"user":{"displayName":"Alexandre Bergel","userId":"02563535896661050156"},"user_tz":-120},"id":"Xuyp-xM_leVN"},"outputs":[],"source":["# Aqui vamos a crear nuestros tipos, esto nos ayudara a entender que hace\n","# cada metodo y funcion que usemos mas claramente.\n","Action = NewType('Action', int)\n","GridElement = NewType('GridElement', str)"]},{"cell_type":"markdown","metadata":{"id":"GApT6pfUdZZD"},"source":["Arriba hemos definimos nuestro mapa. Esto no es importante para entender el concepto de *reinforcement learning*, sino mas bien es una pura implementacion a mano de un mapa. Puedes saltarte toda la parte donde creamos la grilla."]},{"cell_type":"code","execution_count":127,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1725892523442,"user":{"displayName":"Alexandre Bergel","userId":"02563535896661050156"},"user_tz":-120},"id":"j36sWY4KMzQy"},"outputs":[],"source":["# Aqui definimos nuestra grilla que acturara como la base de nuestro mapa\n","class Grid:\n","    # Nuestro constructor toma una lista u otra grilla y la guarda\n","    # se preocupa de copiarla para que no modifiquemos la anterior\n","    def __init__(self, grid:Union['Grid', List[List[GridElement]]]=None) -> None:\n","        assert grid is not None\n","        if isinstance(grid, list):\n","            self.grid = deepcopy(grid)\n","        elif isinstance(grid, Grid):\n","            self.grid = deepcopy(grid.grid)\n","\n","        # Guardamos el tamaÃ±o de la grilla para trabajar mas rapido\n","        self.x_lim = len(self.grid[0])\n","        self.y_lim = len(self.grid)\n","\n","    # Nuestro metodo para comparar una grilla con otra\n","    def __eq__(self, other:'Grid') -> bool:\n","        return isinstance(other, Grid) and self.grid == other.grid\n","\n","    # Simpre es importante que si modificamos nuestra igualdad, tambien\n","    # adaptemos nuestro hash\n","    def __hash__(self) -> int:\n","        return hash(str(self.grid))\n","\n","    # Cuando imprimimos una grilla, esta se mostrara como un\n","    # mapa, como una matriz\n","    def __str__(self) -> str:\n","        return '\\n'.join([' '.join(str(e) for e in row) for row in self.grid])\n","\n","    # Este es un metodo muy util para indexar partes de la grilla\n","    def __getitem__(self, position:Tuple[int, int]) -> GridElement:\n","        assert type(position) == tuple\n","        # necesitamos 2 coordenadas para saber que hay en esa posicion\n","        assert len(position) == 2\n","        x, y = position\n","        # verificamos que las coordenadas esten dentro de la grilla\n","        assert 0 < x <= self.x_lim\n","        assert 0 < y <= self.y_lim\n","        # retornamos el elemento que hay en esa posicion\n","        return self.grid[self.y_lim-y][x-1]\n","\n","    # Este es un metodo muy util para insertar elementos en la grilla\n","    def __setitem__(self, position:Tuple[int, int], value:GridElement) -> None:\n","        assert type(position) == tuple\n","        assert len(position) == 2\n","        x, y = position\n","        assert 0 < x <= self.x_lim\n","        assert 0 < y <= self.y_lim\n","        # igual que antes, pero ahora asignamos un elemento en vez de retornarlo\n","        self.grid[self.y_lim-y][x-1] = value\n","\n","    # Una forma `fancy` de acceder a variables de una clase sin un `getter`\n","    @property\n","    def shape(self) -> Tuple[int, int]:\n","        return (self.x_lim, self.y_lim)"]},{"cell_type":"markdown","metadata":{"id":"4Rx9v-DPixO-"},"source":["Lo anterior pueden ignorarlo completamente, pero ahora si parte lo que nos importa!\n","\n","A continuacion definiremos una clase `State`, la cual representa el estado de nuestro agente en algun momento de su travesia.\n","Un estado tiene 3 cosas:\n","\n","\n","1.   El mapa en ese momento. Es decir, tenemos una grilla dentro del estado. Esto representara el estado del mapa en cada instante.\n","2.   Las posiciones de nuestro **heroe**, claramente necesitamos saber donde esta nuestro personaje en cada momento.\n","3.   Los limites del mapa, para no salirnos del mapa.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":128,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1725892523442,"user":{"displayName":"Alexandre Bergel","userId":"02563535896661050156"},"user_tz":-120},"id":"3L-t7bAufqw4"},"outputs":[],"source":["# La clase `State` representara un estado del heroe\n","\n","class State:\n","    # En nuestro constructor solo asignamos nuestras variables\n","    def __init__(self, grid:Union[Grid, List[List[GridElement]]]=None,\n","                 hero_pos:Tuple[int,int]=(1,1)) -> None:\n","\n","        self.grid = Grid(grid=grid)\n","        self.x_lim, self.y_lim = self.grid.shape\n","        self.hero_x, self.hero_y = hero_pos\n","        self.trophies_collected = 0\n","        \n","        self.has_key = False  # Variable para verificar si el hÃ©roe tiene una llave pregunta 3\n","        self.has_sword = False  # Nueva variable para verificar si el hÃ©roe tiene la espada pregunta 4\n","        PENALTY_THRESHOLD = 3  # NÃºmero de repeticiones antes de penalizar\n","        PENALTY_VALUE = -10    # Valor de la penalizaciÃ³n\n","\n","    # Misma forma `fancy` de acceder a la posicion del heroe\n","    @property\n","    def hero_pos(self) -> Tuple[int, int]:\n","        return (self.hero_x, self.hero_y)\n","\n","    # Para dibujar nuestro mapa con el heroe en la posicion actual\n","    def __str__(self) -> str:\n","        grid = deepcopy(self.grid)\n","        grid[self.hero_x, self.hero_y] = HERO\n","        return grid.__str__()\n","\n","    # Un estado es igual a otro si las grillas y posiciones de los heroes son\n","    # las mismas\n","    def __eq__(self, other:'State') -> bool:\n","        return isinstance(other, State) and self.hero_pos == other.hero_pos and \\\n","            self.grid == other.grid\n","\n","    # Igual que antes, por completitud debemos implementar cuando 2\n","    # estados tienen el mismo hash\n","    def __hash__(self) -> int:\n","        return hash(str(self.grid) + str(self.hero_pos))\n","\n","    # Este metodo nos ayuda a obtener que elemento se encuentra en una posicion\n","    # determinada. Necesitamos el estado pasado para comparar ya que no tenemos\n","    # historia, pero es una forma simple de implementar el mapa sin\n","    # mucho codigo. Recuerda que estamos en una cadena de Markov, aqui no tenemos\n","    # los estados pasados! Estos no afectan la decision que tomaremos ahora.\n","    def get_element(self, position:Tuple[int,int], state:'State') -> GridElement:\n","        assert type(position) == tuple\n","        assert len(position) == 2\n","        x, y = position\n","        assert 0 < x <= self.x_lim\n","        assert 0 < y <= self.y_lim\n","\n","\n","        # Por limitaciones de la implementacion, debemos saber si el heroe se\n","        # movio a la posicion en la que esta, o estaba ahi desde antes\n","        # otra implementacion podria solucionar este problema de mejor manera\n","        # pero es mas compleja de entender\n","        if position == state.hero_pos:\n","            return HERO\n","        return self.grid[x,y]\n","\n","    # De nuestras acciones tenemos que elegir una y actuar acorde a ella.\n","    # por ejemplo, si le pedimos al estado que suba, entonces tenemos que\n","    # enviar la accion `UP`.\n","    # Cuando llamamos a este metodo, creamos un nuevo estado con la\n","    # accion aplicada\n","    def action_dispatch(self, action:Action) -> 'State':\n","        if action == UP:\n","            return self.moveUp()\n","        elif action == DOWN:\n","            return self.moveDown()\n","        elif action == LEFT:\n","            return self.moveLeft()\n","        elif action == RIGHT:\n","            return self.moveRight()\n","        else:\n","            raise ValueError(f\"Unknown action {action}\")\n","\n","    # Este metodo solo copia el estado actual y crea uno nuevo para aplicar\n","    # los cambios pedidos por la accion ingresada\n","    def register(self) -> 'State':\n","        past_state = copy(self)\n","        return State(grid=past_state.grid, hero_pos=past_state.hero_pos)\n","\n","    # Los siguientes metodos mueven nuestro personaje en las direcciones\n","    # que definimos antes, arriba, abajo, derecha e izquierda\n","\n","    def moveUp(self) -> 'State':\n","        new_state = self.register()\n","        new_state.hero_y = new_state.hero_y + 1 if new_state.hero_y < new_state.y_lim else new_state.hero_y\n","\n","        return new_state\n","\n","    def moveDown(self) -> 'State':\n","        new_state = self.register()\n","        new_state.hero_y = new_state.hero_y - 1 if new_state.hero_y > 1 else new_state.hero_y\n","\n","        return new_state\n","\n","    def moveRight(self) -> 'State':\n","        new_state = self.register()\n","        new_state.hero_x = new_state.hero_x + 1 if new_state.hero_x < new_state.x_lim else new_state.hero_x\n","\n","        return new_state\n","\n","    def moveLeft(self) -> 'State':\n","        new_state = self.register()\n","        new_state.hero_x = new_state.hero_x - 1 if new_state.hero_x > 1 else new_state.hero_x\n","\n","        return new_state"]},{"cell_type":"code","execution_count":129,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1725892523442,"user":{"displayName":"Alexandre Bergel","userId":"02563535896661050156"},"user_tz":-120},"id":"cbMhqHTpqDEs","outputId":"8934fb86-5e32-406c-edd7-6664d07ec897"},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ† âšª âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","ğŸ™ƒ âšª âšª\n"]}],"source":["# Creamos una lista de listas (una matriz) que represente a nuestro mapa\n","mapa_ejemplo = [\n","    [TROPHIE, EMPTY, EMPTY],\n","    [ZOMBIE, ZOMBIE, EMPTY], \n","    [EMPTY, EMPTY, EMPTY]\n","]\n","# Digamos que nuestro heroe parte en la posicion (1,1)\n","estado_ejemplo = State(grid=mapa_ejemplo, hero_pos=(1, 1))\n","\n","# Veamos como se ve!\n","print(estado_ejemplo)"]},{"cell_type":"markdown","metadata":{"id":"g70vnhBcm1Bd"},"source":["Listo, con esto hemos definido las bases para que nuestro **agente** pueda moverse libremente por el mundo que creemos.\n","Probemos a ver como funciona!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Xw4LeuzVqCBE"},"source":["Ahi esta nuestro **heroe** ğŸ™ƒ! Tambien podemos ver nuestro trofeo ğŸ† a cual queremos llegar, y los zombies ğŸ§Ÿ que debemos evitar en el camino. Vamos a representar el camino libre como un circulo blanco âšª.\n","\n","Ahora empieza la parte de aprendizaje."]},{"cell_type":"markdown","metadata":{"id":"e9xgY1HWnBRJ"},"source":["## Feedback para el **agente**\n","\n","Hemos creado nuestro estado, nuestro mapa y todo, pero ahora necesitamos de alguna forma ver que acciones merecen un premio para el **agente** y cuales un castigo.\n","\n","- [x] Mapa\n","- [x] Definicion de un estado\n","- [ ] Cuando dar recompensas y cuando castigar\n","- [ ] Aprender...\n","\n","Esto lo definiremos en una funcion que llamaremos `act`. La funcion `act` necesita un `State` `s` y un `Action` `a` como argumentos, para simular un movimiento, desde un estado `s` mediante la accion `a`."]},{"cell_type":"markdown","metadata":{"id":"QVZoxljBvHrE"},"source":["## Listos para aprender!\n","\n","Ahora tenemos todo nuestro ambiente implementado y definido.\n","\n","- [x] Mapa\n","- [x] Definicion de un estado\n","- [x] Cuando dar recompensas y cuando castigar\n","- [ ] Aprender...\n","\n","Lo que necesitamos ahora es algun algoritmo que nos ayude a aprender que acciones son buenas y cuales no. Para esto usaremos un algoritmo llamado *Q-Learning*, que es lo que vimos en la clase teorica antes.\n","\n","Para esto, primero necesitamos una tabla donde iremos guardando cada uno de los estados y sus puntajes para cada accion. Es decir, dado un estado `s`, que deberiamos hacer ahora. Esta decision se toma de acuerdo a un puntaje que va asociado a cada accion dado cada estado."]},{"cell_type":"code","execution_count":130,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1725892523442,"user":{"displayName":"Alexandre Bergel","userId":"02563535896661050156"},"user_tz":-120},"id":"LPzvQBy1wuQs"},"outputs":[],"source":["# Declaramos nuestra tabla `q_table` como un diccionario vacio\n","# Nuestra tabla se vera de la siguiente forma:\n","# estado: [lista de acciones posibles]\n","# Esta lista de acciones posibles es una lista de puntajes para cada\n","# accion dado un estado.\n","q_table = {}\n","\n","# Luego hacemos nuestra funcion de busqueda `q`.\n","# Esta tiene 2 funciones:\n","# 1. Dado  un estado, retorna una lista con los puntajes para cada accion en ese\n","# estado. Es decir, una lista con puntajes para decidir que hacer\n","# 2. Dado un estado y una accion, retorna el puntaje asociado a realizar\n","# esa accion en ese estado.\n","def q(state:State, action:Action=None) -> Union[float, np.ndarray]:\n","    if state not in q_table:\n","        # Si no hemos visto este estado, lo creamos\n","        # como no sabemos que hacer aun, decimos que todas las acciones\n","        # tienen beneficio 0, ya que no lo hemos evaluado aun\n","        #q_table[state] = np.zeros(len(ACTIONS))\n","        q_table[state] = np.ones(len(ACTIONS)) * 1\n","    if action is None:\n","        return q_table[state]\n","\n","    return q_table[state][action]\n","\n","# Este es un metodo conveniente para no estar borrando manualmente\n","# la tabla cada vez que queremos hacer algo nuevo\n","def reset_table():\n","    q_table = {}"]},{"cell_type":"code","execution_count":131,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1725892523442,"user":{"displayName":"Alexandre Bergel","userId":"02563535896661050156"},"user_tz":-120},"id":"nPi4H0QPMzTx"},"outputs":[],"source":["# Definimos una funcion que represente un `acto`. Es decir\n","# dado un estado y una accion, que ocurre.\n","# Este \"que ocurre\" es bastante variado, podemos movernos,\n","# ganar puntaje, perder el juego, o cualquier cosa que decidamos\n","\n","# Aqui es donde debemos decidir cuando y cuanta recompensa o castigo\n","# debemos dar a nuestro agente.\n","# Esta funcion retornara 3 cosas. El nuevo estado en que quedo nuestro heroe,\n","# una recompensa por su esfuerzo (puede ser negativa), y un booleano indicando\n","# si el juego termino o no. Este `termino` puede ser porque ganamos o perdimos.\n","\n","def act(state:State, action:Action):\n","   \n","    # Le decimos a nuestro estado que se mueva en la direccion pedida\n","    # esto nos da un nuevo estado\n","    new_state = state.action_dispatch(action)\n","\n","    # ahora le pedimos al nuevo estado que nos diga que hay\n","    # en la posicion que quedamos\n","    # De nuevo, por un tema de implementacion tenemos que saber si donde estamos\n","    # ahora estaba ocupado por otro elemento, o si siempre estuvimos nosotros ahi.\n","    grid_item = new_state.get_element(new_state.hero_pos, state)\n","\n","    # Si nos encontramos un zombie, que hacemos?\n","    if grid_item == ZOMBIE:\n","        # como no tenemos como defendernos, perdemos\n","        # le daremos una recompensa negativa al agente por que se equivoco\n","\n","        # le daremos -100 como recompensa.\n","        reward = -100\n","        # el juego se acabo...\n","        is_done = True\n","   \n","    # Si nos encontramos con un trofeo, que hacemos?\n","    elif grid_item == TROPHIE:\n","        new_state.trophies_collected += 1  # Recogemos el trofeo\n","        # pues ganamos! Le damos de recompensa al agente\n","        reward = 100\n","       # El juego termina si se han recogido ambos trofeos\n","        if new_state.trophies_collected == 2:  #Se agrega pregunta 2\n","                reward += 1000  # Recompensa adicional por el segundo trofeo\n","                is_done = True # el juego termina porque encontro los dos trofeos \n","        else:\n","            is_done = False\n","            \n","    # Si el espacio esta vacio, que hacemos?\n","    elif grid_item == EMPTY:\n","        # nada, simplemente nos movemos a ese lugar.\n","        # por que es negativa la recompensa? Lo veremos en la pregunta 7!\n","        reward = -1\n","        # no se ha terminado el juego\n","        is_done = False\n","    \n","    # Si el heroe ya estba en ese espacio, que hacemos?\n","    elif grid_item == HERO:\n","        # nada, simplemente nos quedamos igual.\n","\n","        # por que es negativa la recompensa? Lo veremos en la pregunta 7!\n","        reward = -1\n","        # no se ha terminado el juego\n","        is_done = False\n","\n","        # Si nos encontramos un bloque pregunta 3\n","    elif grid_item == BLOCK:\n","        reward = -10  # PenalizaciÃ³n por chocar con un bloque\n","        is_done = False  # El juego continÃºa\n","\n","    # Si nos encontramos una puerta pregunta 3\n","    elif grid_item == DOOR:\n","        if new_state.has_key:\n","            new_state.grid[new_state.hero_pos] = EMPTY  # La puerta desaparece una vez abierta\n","            reward = 100  # Recompensa por abrir la puerta\n","        else:\n","            reward = -10  # PenalizaciÃ³n por no tener la llave\n","        is_done = False\n","\n","    # Si nos encontramos una llave pregunta 3\n","    elif grid_item == KEY:\n","        new_state.has_key = True  # El hÃ©roe obtiene la llave\n","        reward = 100  # Recompensa por conseguir la llave\n","        new_state.grid[new_state.hero_pos] = EMPTY  # La llave desaparece una vez recogida\n","        is_done = False\n","\n","     # Si nos encontramos una espada pregunta 4\n","    elif grid_item == SWORD:\n","        new_state.has_sword = True  # El hÃ©roe obtiene la espada\n","        reward = 100  # Recompensa por conseguir la espada\n","        new_state.grid[new_state.hero_pos] = EMPTY  # La espada desaparece una vez recogida\n","        is_done = False\n","\n","    else:\n","        raise ValueError(f\"Unknown grid item {grid_item}\")\n","\n","    return new_state, reward, is_done"]},{"cell_type":"markdown","metadata":{"id":"0q9_KqVHNFFO"},"source":["Para terminar nuestras configuraciones, es necesario que digamos cuantas veces intentaremos correr el juego, y tambien la duracion del juego. Por ejemplo, podriamos estar jugando 1 hora por 2 semanas, o 14 horas en un puro dia. Para asimilar lo aprendido, primero debemos dormir, es decir, dejar de jugar.\n","\n","Este ejemplo de dormir y tiempo entre juegos es una analogia muy util para describir el concepto de `episodios` y `pasos` de nuestro **agente**. Por cada `episodio` nuestro agente comienza el juego denuevo y podemos darnos cuenta si mejoro o no, entonces no nos sirve solo hacer un `episodio` super largo si no tendremos la oportunidad de verificar los resultados; pero si tenemos demasiados episodios, no terminaremos de jugar nunca. El \"largo\" del juego viene dado por los `pasos`. Si son muy pocos `pasos` puede que no alcancemos a aprender lo que queremos, pero si son muchos puede que estemos perdiendo el tiempo y ya hayamos encontrado lo que buscabamos.\n","\n","A continuacion definiremos estas constantes para nuestro problema."]},{"cell_type":"code","execution_count":132,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1725892523442,"user":{"displayName":"Alexandre Bergel","userId":"02563535896661050156"},"user_tz":-120},"id":"Ot-A9d6kMzYK"},"outputs":[],"source":["# El total de episodios donde nuestro agente aprendera\n","N_EPISODES = 20\n","\n","# El maximo numero de pasos por episodio\n","MAX_EPISODE_STEPS = 10\n","\n","# Debemos definir nuestro conjunto de pesos de entrenamiento.\n","# En un comienzo nuestro agente aprendera mucho, ya que sus primeros\n","# acercamientos al juego son mas valiosos. Pero mientras mas veces jugamos\n","# lo que aprendemos por cada jugada es cada vez menos. Es importante hacer esta\n","# diferencia o una jugada muy avanzada, por intentar explorar, podria\n","# arruinar todo lo que habiamos aprendido antes.\n","\n","# Siempre aprenderemos aun que sea un poco\n","MIN_ALPHA = 0.02\n","# Aprenderemos desde TODO, hasta un 2% de lo que veamos.\n","# Esta decision es arbitraria, intenten cambiarlo a ver que pasa!\n","alphas = np.linspace(1.0, MIN_ALPHA, N_EPISODES)\n","\n","# Un factor de descuento. Esto lo usamos para balancear entre la recompensa\n","# maxima a corto plazo, o a largo plazo. Si lo dejamos solo a corto plazo\n","# es poco probable que aprendamos algo util a futuro. Pero si lo dejamos en\n","# 100% entonces estamos pensando demasiado en el futuro y no nos estamos\n","# preocupando del presente.\n","# Generalmente este valor esta entre 80 y 99%\n","gamma = 0.9\n","\n","# Si solo nos guiamos por la mejor accion y no exploramos ni nos arriesgamos\n","# es poco probable que aprendamos mucho del mundo. Por esto, es importante\n","# poner un poco de aleatoriedad en esto. Existe un 20% de probabilidades\n","# de que elijamos una accion al azar dado el estado que estamos. En contraste\n","# existe un 80% de probabilidad de que elijamos la mejor accion que conocemos.\n","eps = 0.2\n","\n","\n","# Aqui simulamos la eleccion de una accion. Dado un estado, nos dice que\n","# accion tomar. Existe un `eps` probabilidad de que tomemos una accion\n","# al azar.\n","def choose_action(state:State) -> Action:\n","    if random.random() < eps:\n","        return random.choice(ACTIONS)\n","    else:\n","        return np.argmax(q(state))"]},{"cell_type":"markdown","metadata":{"id":"o83QfpM9S6Cr"},"source":["Listo! Implementemos `Q-learning` para que nuestro heroe ğŸ™ƒ aprenda como obtener los trofeos ğŸ†!\n","\n","MaÃ±ana veremos los detalles de como nace esta ecuacion basado en las *cadenas de Markov*, pero por mientras aqui les presentamos la formula y que significa cada termino de ella. La formula para aprender mediante `Q-learning` viene dada por la siguiente expresion:\n","\n","$$ Q^{nuevo}(s_t, a_t) =\n","    \\underbrace{Q(s_t, a_t)}_{\\text{valor antiguo}} +\n","    \\underbrace{\\alpha}_{\\text{la tasa de aprendizaje}} *\n","        \\overbrace{\n","            \\left(\\underbrace{r_t}_{\\text{recompensa}} +\n","            \\underbrace{\\gamma}_{\\text{factor descuento}} *\n","                \\underbrace{\\max_{acciones}(Q(s_{t+1},acciones))}_{\\text{valor optimo futuro}} -\n","                \\underbrace{Q(s_t, a_t)}_{\\text{diferencia temporal}}\n","            \\right)}\n","        ^{\\text{lo que aprendimos}}\n","$$\n","\n","Basicamente, para el estado actual, debemos ajustar el valor que nos dice que accion tomar basandonos en cual creemos que es el estado que nos da una mayor recompensa en el futuro. Es decir, desde el estado donde estamos, que accion nos lleva a un estado de mayor recompensa. El termino $Q(s_t, a_t)$ es importante porque representa la diferencia temporal del estado actual con el siguiente. No hay garantia que visitar 2 veces el mismo estado nos de la misma recompensa, por lo que hay que considerar que existe el tiempo en nuestra ecuacion.\n","\n","En el siguiente trozo de codigo implementamos esta ecuacion en un loop `for`. Para cada numero de `episodios`, avanzamos/ejecutamos hasta que el juego termina (porque ganamos o perdimos) o hasta que alcancemos el maximo de `pasos`. Es importante seÃ±alar que no todos los juegos *terminan*, asi que es importante que tengamos un limite hasta cuando queremos seguir. Tambien, si nos quedamos parados en el mismo lugar sin movernos, y no hay condiciones de tiempo, el juego durara para siempre. Hay que arreglar esos detalles."]},{"cell_type":"code","execution_count":133,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1725892523442,"user":{"displayName":"Alexandre Bergel","userId":"02563535896661050156"},"user_tz":-120},"id":"8ITT2755S6LH"},"outputs":[],"source":["def q_learning(start_state:State, episodes:int, steps:int,\n","               table:Dict[State, np.ndarray], learning_rate:np.ndarray,\n","               discount:float) -> None:\n","\n","    for ep in range(episodes):\n","\n","        # Creamos una copia para no modificar nuestro estado original\n","        state = deepcopy(start_state)\n","\n","        # Partimos con una recompensa 0\n","        total_reward = 0\n","\n","        # Cada episodio tiene una tasa de aprendizaje distinto\n","        alpha = learning_rate[ep]\n","\n","        # Para cada paso, vamos a ir actualizando nuestra tabla\n","        # para encontrar los movimientos que nos llevaran a ganar el juego\n","        for _ in range(steps):\n","\n","            # Tomamos una accion de nuestro banco de acciones\n","            # dado nuestro estado\n","            action = choose_action(state)\n","\n","            # Llamamos un `acto`, para ver si lo hicimos bien\n","            # necesitamos indicarle el estado donde estamos y la accion a\n","            # realizar\n","            # Esto nos da un nuevo estado, una recompensa y nos dice\n","            # si se termino el juego o no.\n","            next_state, reward, done = act(state, action)\n","\n","            # Vamos guardando nuestras recompensas\n","            total_reward += reward\n","\n","            # Actualizamos nuestros estados con la formula que vimos antes\n","            q(state)[action] = q(state, action) + \\\n","                alpha * (reward + gamma * np.max(q(next_state)) - q(state, action))\n","\n","            # estamos listos para el siguiente paso\n","            state = next_state\n","\n","            # si el juego termino, dejamos los pasos y comenzamos con un\n","            # nuevo episodio\n","            if done:\n","                break\n","\n","        print(f\"Episode {ep + 1}: total reward -> {total_reward}\")"]},{"cell_type":"markdown","metadata":{"id":"qshfYj-VqKJm"},"source":["## Listos para jugar!\n","\n","Ahora que ya tenemos nuestro estado, nuestro mapa, sabemos cuando darle recompensas a nuestro **agente**, y ademas implementamos la formula de arriba para aprender, solo nos queda ejecutar nuestro programa y ver como nuestro heroe ğŸ™ƒ esquiva los zombies ğŸ§Ÿ para obtener el trofeo ğŸ†!\n","\n","- [x] Mapa\n","- [x] Definicion de un estado\n","- [x] Cuando dar recompensas y cuando castigar\n","- [x] Aprender...\n","\n","Creemos nuestro mapa :-)"]},{"cell_type":"code","execution_count":134,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1725892523442,"user":{"displayName":"Alexandre Bergel","userId":"02563535896661050156"},"user_tz":-120},"id":"Ca_6LCApMzbJ","outputId":"c1bc2d94-13bd-42fe-e8a9-b62e236bfbd6"},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ† ğŸ† âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","ğŸ™ƒ âšª âšª\n"]}],"source":["# Dibujamos nuestro mapa. Pueden dibujar lo que quieran, a ver si el\n","# agente se la puede\n","\n","#se coloca segundo trofeo pregunta 2\n","grid = [\n","    [TROPHIE, TROPHIE, EMPTY],\n","    [ZOMBIE, ZOMBIE,EMPTY ], \n","    [EMPTY, EMPTY, EMPTY]\n","]\n","\n","#grid = [\n","#   [TROPHIE, EMPTY, EMPTY, EMPTY, ZOMBIE, TROPHIE, BLOCK, BLOCK],\n","#   [ZOMBIE, ZOMBIE, BLOCK, EMPTY, BLOCK, BLOCK, BLOCK, BLOCK],\n","#    [DOOR, EMPTY, EMPTY, EMPTY, BLOCK, BLOCK, BLOCK, BLOCK],\n","#    [TROPHIE, BLOCK, EMPTY, EMPTY, BLOCK, BLOCK, BLOCK, BLOCK],\n","#   [ZOMBIE, EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, BLOCK],\n","#    [EMPTY, EMPTY, BLOCK, EMPTY, BLOCK, BLOCK, EMPTY, BLOCK],\n","#    [EMPTY, ZOMBIE, BLOCK, KEY, BLOCK, BLOCK, EMPTY, BLOCK],\n","#   [EMPTY, EMPTY, EMPTY, BLOCK, BLOCK, BLOCK, SWORD, BLOCK]\n","#]\n","\n","\n","# Y aqui definimos nuestro estado inicial. Pueden poner al heroe donde\n","# quieran, pero cuiden que no parte sobre un zombie!\n","initial_state = State(grid=grid, hero_pos=(1, 1))\n","\n","print(initial_state)\n"]},{"cell_type":"markdown","metadata":{"id":"PTbdoDpls8xS"},"source":["## Ejecutando las guias de aprendizaje\n","\n","A continuacion vamos a llamar a la funcion que hara que nuestra `q_table` se llene de informacion util y representativa del juego que queremos ganar."]},{"cell_type":"code","execution_count":135,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":665,"status":"ok","timestamp":1725892524102,"user":{"displayName":"Alexandre Bergel","userId":"02563535896661050156"},"user_tz":-120},"id":"0RVj4G6bwDWx","outputId":"aa90b5c9-b506-4d3b-f4d2-8cd9df55a829"},"outputs":[{"name":"stdout","output_type":"stream","text":["Episode 1: total reward -> -100\n","Episode 2: total reward -> -103\n","Episode 3: total reward -> -10\n","Episode 4: total reward -> -10\n","Episode 5: total reward -> -104\n","Episode 6: total reward -> 91\n","Episode 7: total reward -> -106\n","Episode 8: total reward -> -5\n","Episode 9: total reward -> 192\n","Episode 10: total reward -> 293\n","Episode 11: total reward -> -103\n","Episode 12: total reward -> 293\n","Episode 13: total reward -> 293\n","Episode 14: total reward -> 293\n","Episode 15: total reward -> 293\n","Episode 16: total reward -> 293\n","Episode 17: total reward -> 192\n","Episode 18: total reward -> -101\n","Episode 19: total reward -> 192\n","Episode 20: total reward -> 293\n"]}],"source":["# por si acaso, antes de entrenar reiniciamos nuestra tabla para no tener\n","# informacion demas\n","reset_table()\n","\n","# Le pasamos los argumentos a nuestra funcion de aprendizaje y estamos\n","# listos para ver los resultados de nuestro agente\n","q_learning(start_state=initial_state,\n","           episodes=N_EPISODES,\n","           steps=MAX_EPISODE_STEPS,\n","           table=q_table,\n","           learning_rate=alphas,\n","           discount=gamma)"]},{"cell_type":"markdown","metadata":{"id":"F_H0UslQxm3b"},"source":["## Viendo los resultados\n","\n","Bueno, la funcion de antes me mostro algunos numeros pero se si en verdad el heroe ğŸ™ƒ habra logrado su comentido!\n","Ejecuta el codigo de abajo para que se muestre lo aprendido."]},{"cell_type":"code","execution_count":136,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":108,"status":"ok","timestamp":1725892524102,"user":{"displayName":"Alexandre Bergel","userId":"02563535896661050156"},"user_tz":-120},"id":"dgAnCFsOxl3V","outputId":"7d969365-e9cd-4c91-ae3d-6d2f4a3ebe4c"},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ† ğŸ† âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","ğŸ™ƒ âšª âšª\n","up=-99.1, down=-2.466374159737405, left=-2.188283536797593, right=50.08568010070059\n","ğŸ† ğŸ† âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª ğŸ™ƒ âšª\n","up=-97.69895632622831, down=-1.4659387755602857, left=-1.9468172126589627, right=91.57763367639937\n","ğŸ† ğŸ† âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª ğŸ™ƒ\n","up=169.71645138199412, down=-0.7812799113573409, left=-1.0245337735425113, right=-0.7809872686980611\n","ğŸ† ğŸ† âšª\n","ğŸ§Ÿ ğŸ§Ÿ ğŸ™ƒ\n","âšª âšª âšª\n","up=259.102362926574, down=23.52696442028242, left=-88.44780720221607, right=-0.5720523545706373\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=5.60148804543899, down=-62.958631578947376, left=1.0, right=302.61456402068467\n","ğŸ† ğŸ† ğŸ™ƒ\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n","up=83.63867899602502, down=17.18385479453216, left=354.6035435071733, right=93.19503236409399\n","ğŸ† ğŸ™ƒ âšª\n","ğŸ§Ÿ ğŸ§Ÿ âšª\n","âšª âšª âšª\n"]}],"source":["# Solo en caso de que nuestro heroe se quede pegado, ponemos un maximo\n","# de escenarios a mostrar, aqui tenemos maximo 100\n","show_max = 100\n","\n","# Partimos con el estado inicial, preguntemos a donde podemos movernos\n","possible_actions = q(initial_state)\n","print(initial_state)\n","print(f\"up={possible_actions[UP]}, \"\n","      f\"down={possible_actions[DOWN]}, \"\n","      f\"left={possible_actions[LEFT]}, \"\n","      f\"right={possible_actions[RIGHT]}\")\n","\n","# Seleccionamos la accion con el mejor puntaje, y le pedimos al\n","# heroe que se mueva en esa direccion. Esto nos da un nuevo estado\n","s, _, done = act(initial_state, np.argmax(possible_actions))\n","\n","# Mientras no haya terminado el juego, o nos hayamos pasado del maximo de\n","# acciones a mostar definido mas arriba, seguimos moviendonos con la\n","# accion mas favorable para ese estado\n","while not done and show_max:\n","    # Mostramos el estado actual\n","    print(s)\n","    # vemos nuestras acciones\n","    possible_actions = q(s)\n","    # Que accion deberiamos tomar?\n","    print(f\"up={possible_actions[UP]}, \"\n","      f\"down={possible_actions[DOWN]}, \"\n","      f\"left={possible_actions[LEFT]}, \"\n","      f\"right={possible_actions[RIGHT]}\")\n","    # Elejimos la mejor accion y continuamos\n","    s, _, done = act(s, np.argmax(possible_actions))\n","\n","    show_max -= 1\n","print(s)\n"]},{"cell_type":"markdown","metadata":{"id":"vnzUuNcQ1B2u"},"source":["\n","\n","---\n","\n","\n","## Preguntas\n","\n","Esperamos haya sido entretenido ver como nuestro heroe ğŸ™ƒ lograba esquivar los zombies ğŸ§Ÿ para llegar al trofeo ğŸ†. Ahora les toca a ustedes mejorar lo que les mostramos y responder las siguientes preguntas."]},{"cell_type":"markdown","metadata":{"id":"MTm4i4km1gzh"},"source":["### Pregunta 1: haciendo una curva de aprendizaje\n","En esta pregunta les pedimos que dibujen un grafico, usando `matplotlib`, donde se pueda ver como fue cambiando la **recompensa** total por cada episodio que iba pasando. Lo que queremos ver es que la recompensa debe partir baja, pero a medida que pasan los episodios, esta deberia subir hasta que muestre que siempre gana el juego. O quizas suba y baje todo el rato... Por que ocurre esto?\n","\n","> Hagan un grafico de recompensa por episodio."]},{"cell_type":"code","execution_count":137,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"elapsed":106,"status":"error","timestamp":1725892524102,"user":{"displayName":"Alexandre Bergel","userId":"02563535896661050156"},"user_tz":-120},"id":"-9dDMBSkHKpu","outputId":"d9f90f94-63f8-41d7-f39a-c1b33750a870"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# ESCRIBA AQUI SU RESPUESTA A LA PREGUNTA 1\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","# Recompensa total por cada episodio\n","recompensa = [-100, -102, -101, -101, -10, -10, -109, -105, -10, -108, \n","           -10, -102, 991, 994, 994, 991, 994, 995, 994, -101]\n","\n","# Episodios\n","episodes = range(1, len(recompensa) + 1)\n","\n","# Plotting the reward curve\n","plt.plot(episodes, recompensa, marker='o', linestyle='-', color='b')\n","\n","# Adding labels and title\n","plt.xlabel('Episodios')\n","plt.ylabel('Recompensa Total')\n","\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Respuesta:\n"," \n","Se observa que los primeros episodios el modelo estÃ¡ aprendiendo y ya luego del episodio 12 el modelo aprendiÃ³ una estrategia que produce altas recompensas. Este comportamiento esperado en reinforcement learning.  Adicionalmente, es importante tener en cuenta que dependiendo de las constantes que definamos podemos optimizar o no el aprendizaje del modelo."]},{"cell_type":"markdown","metadata":{"id":"KmNDV-oj3B4y"},"source":["### Pregunta 2: solo un trofeo?\n","\n","El juego esta muy simple, obvio que puede lograrlo... Que pasa si agregamos otro trofeo ğŸ†, y la condicion para ganar es que debe recoger **ambos** trofeos ğŸ†ğŸ† para terminar el juego?\n","\n","Completa la clase `State` para implementar este funcionamiento.\n","\n","> Implementen lo necesario para que se necesiten 2 trofeos para terminar el juego. Rellenen el codigo en las secciones anteriores, y modifiquen lo necesario"]},{"cell_type":"markdown","metadata":{"id":"KTqFOk834yCd"},"source":["### Pregunta 3: agreguemos mas cosas!!\n","\n","Nuestro juego ya tiene zombies ğŸ§Ÿ, trofeos ğŸ† y a nuestro heroe ğŸ™ƒ. Ademas, agregamos esta condicion extra para necesitar mas de 1 trofeo para terminar el juego. Es momento de hacerlo mas entretenido.\n","\n","Ahora deberan agregar los siguientes elementos al juego:\n","\n","1.   Bloques ğŸš«: el heroe ğŸ™ƒ no puede pasar por aqui, es lo mismo a chocar con una muralla.\n","2.   Puerta ğŸšª: un tipo de bloqueo, pero que puede ser desbloqueado consiguiendo la llave que abre la puerta. El heroe ğŸ™ƒ no puede pasar por aqui hasta que consigua la llave ğŸ”‘.\n","3.   Llave ğŸ”‘: un objeto que esta en alguna parte del mapa. Una vez conseguimos este objeto podemos abrir una de las puertas del juego.\n","\n","Ayudas e indicaciones:\n","\n","*   Los bloques ğŸš« son lo mismo que una muralla, solo se ven distintos y, claro, no estan en los extremos del mapa. No hay forma de destruirlos y no impactan al jugador, solo estan ahi para estorbar.\n","*   Puedes considerar que una vez abierta la puerta ğŸšª, esta desaparece. No te preocupes de esto ya que no es la idea de la actividad. Solo debes implementar la condicion que la puerta no puede atravesarse hasta conseguir la llave ğŸ”‘.\n","*   Para la llave ğŸ”‘ hay 2 opciones. Puedes considerar que solo se puede usar una vez y luego para abrir una segunda puerta hay que buscar una nueva, o que una sirve para todas. Esto no es lo importante, asi que puedes implementar lo que consideres mas entretenido :-)\n","*   Estos nuevos elementos se llaman `BLOCK` ğŸš«, `DOOR` ğŸšª y `KEY` ğŸ”‘, y fueron definidos al principio de este documento. Debes modificar las funciones que interactuan con estos elementos. En la clase `State` puedes agregar si el heroe tiene ya la llave o no.\n","\n","> Implementen los Bloques ğŸš«, Puertas ğŸšª y Llaves ğŸ”‘ en el juego. Una puerta solo puede abrirse una vez conseguida una llave.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"20QzNGh_-JTK"},"source":["Hasta aqui llega el primer dia! Algunas cosas para que se vayan pensado:\n","\n","*   Funciona con tantos elementos el juego tal y como lo definimos?\n","\n","R: Cada vez que le vamos agregando mas elementos , puede que sea mas dificil el aprendizaje y optimizacion. Es posible que este aprendiendo algunas combinaaciones solamente\n","\n","*   Esta aprendiendo nuestro heroe todas estas mecanicas complejas del juego? Como podriamos ayudar?\n","\n"," R:Parece que el hÃ©roe estÃ¡ aprendiendo parcialmente, pero no todas las mecÃ¡nicas con la misma eficacia. La variabilidad de las recompensas y el hecho de que el hÃ©roe no siempre alcance un desempeÃ±o Ã³ptimo en todos los episodios puede sugerir que algunas mecÃ¡nicas no estÃ¡n siendo aprendidas completamente, o que el agente no estÃ¡ explorando el espacio de acciones correctamente.\n"," \n","Como podriamos ayudar? primero podriamos \n","\n","\n","Desde aqui continuamos: dia 2!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xMDtUWJn8Zx8"},"source":["### Pregunta 4: hora de enfrentarse a los zombies ğŸ§Ÿ\n","\n","Hasta ahora solo hemos evitado a los zombies ğŸ§Ÿ, pero ya no mas! Es hora de hacerles frente, asi que agregaremos una espada ğŸ—¡ï¸ al juego. Los zombies siguen siendo peligrosos al heroe ğŸ™ƒ si esta desarmado, pero una vez el heroe ğŸ™ƒ consigue la espada ğŸ—¡ï¸ puede enfrentarlos. Cuando el heroe consigue la espada ğŸ—¡ï¸, si toca a un zombie este desaparece del mapa y da una recompensa al heroe, algo asi como puntos extra.\n","\n","Ayudas:\n","\n","*   Primero debes agregar la espada ğŸ—¡ï¸ al juego, y luego sumarla a las condiciones de estado de `State`. Es muy similar a la llave ğŸ”‘ y puerta ğŸšª, solo que ahora el zombie ya existia.\n","\n","> Implementen la espada ğŸ—¡ï¸ en el juego. Cuando el heroe ğŸ™ƒ tiene la espada ğŸ—¡ï¸, puede vencer facilmente a los zombies ğŸ§Ÿ."]},{"cell_type":"markdown","metadata":{"id":"viX6Vaoa-KnJ"},"source":["### Pregunta 5: no funciona...\n","\n","Nuestro heroe no esta siendo capaz de aprender a jugar el nuevo juego despue de agregar tantas cosas. Hemos hecho el juego demasiado dificil?\n","\n","> Indiquen que cosas hay que cambiar en las configuraciones del aprendizaje para que ahora si podamos ganar. Intenten con el mapa que dejamos mas abajo."]},{"cell_type":"markdown","metadata":{"id":"wLGLEy0q_E98"},"source":["Respuesta a **pregunta 5**.\n","\n","Escriba aqui su respuesta..."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":105,"status":"aborted","timestamp":1725892524103,"user":{"displayName":"Alexandre Bergel","userId":"02563535896661050156"},"user_tz":-120},"id":"zQynZn5bCQBz"},"outputs":[],"source":["grid = [\n","    [TROPHIE, EMPTY, EMPTY, EMPTY, ZOMBIE, TROPHIE, BLOCK, BLOCK],\n","    [ZOMBIE, ZOMBIE, BLOCK, EMPTY, BLOCK, BLOCK, BLOCK, BLOCK],\n","    [DOOR, EMPTY, EMPTY, EMPTY, BLOCK, BLOCK, BLOCK, BLOCK],\n","    [TROPHIE, BLOCK, EMPTY, EMPTY, BLOCK, BLOCK, BLOCK, BLOCK],\n","    [ZOMBIE, EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, BLOCK],\n","    [EMPTY, EMPTY, BLOCK, EMPTY, BLOCK, BLOCK, EMPTY, BLOCK],\n","    [EMPTY, ZOMBIE, BLOCK, KEY, BLOCK, BLOCK, EMPTY, BLOCK],\n","    [EMPTY, EMPTY, EMPTY, BLOCK, BLOCK, BLOCK, SWORD, BLOCK]\n","]\n","print(Grid(grid))"]},{"cell_type":"markdown","metadata":{"id":"KUOKT2fh_KR6"},"source":["### Pregunta 6: no funcionaba... por que?\n","\n","> Por que hubo que cambiar esos parametros en la pregunta 5? Den un comentario respecto a lo que hacen esos parametros y por que cambiarlos arreglo todo nuestro problema\n","\n","Ayudas:\n","\n","*   Cuando juegas un juego, entiendes todo a la primera, o hay que intentarlo varias veces para acordarse?\n","*   Al jugar algo, o aprender una nueva habilidad, hay que practicarlo miles de veces 1 segundo, o unas 100 veces pero mas tiempo?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ucRN_Ugw_hCS"},"source":["Respuesta a **pregunta 6**.\n","\n","Escriba aqui su respuesta..."]},{"cell_type":"markdown","metadata":{"id":"pYwrztUy_wyx"},"source":["### Pregunta 7: Recompensa negativa al movernos\n","\n","Durante todo este tiempo, en la funcion `act`, cada vez que nuestro heroe se movia a un lugar vacio, es decir, se movia respetando todas las reglas, le dabamos una recompensa de `-1`. Especificamente asi:\n","```python\n","...\n","elif grid_item == EMPTY:\n","    reward = -1\n","    is_done = False\n","...\n","```\n","Esto quiere decir que hemos estado castigando al heroe cada vez que se mueve... Por que? Podriamos borrar esa condicion y dar una recompensa positiva? Una recompensa de 0? Comenten sobre que opinan de esto y para que creen que sirve.\n","\n","> Por que la recompensa de moverse es negativa? Que pasaria si la cambiamos a una recompensa positiva, o 0?"]},{"cell_type":"markdown","metadata":{"id":"sEc8J5D1AzUJ"},"source":["Respuesta a **pregunta 7**.\n","\n","Escriba aqui su respuesta..."]},{"cell_type":"markdown","metadata":{"id":"AlTKr1F4A0tj"},"source":["### Pregunta Extra: murallas fijas\n","\n","Siempre hemos considerado que las murallas del mapa son fijas y no podemos cruzarlas, pero, que ocurre si pudieramos atravesarlas y aparecer al otro lado? Implemente este cambio y cuente sus resultados.\n","\n","> Implemente el poder atravesar uno de los limites del mapa y aparecer en el otro lado.\n","\n","Aclaracion: esta es una pregunta opcional y no es requerido responderla para la nota (pero puede haber algo extra ;-).\n"]},{"cell_type":"markdown","metadata":{"id":"DoyiCywHBQUJ"},"source":["Respuesta a **pregunta extra**.\n","\n","Escriba aqui su respuesta..."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":105,"status":"aborted","timestamp":1725892524103,"user":{"displayName":"Alexandre Bergel","userId":"02563535896661050156"},"user_tz":-120},"id":"69rSAU-uAyG_"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"}},"nbformat":4,"nbformat_minor":0}
